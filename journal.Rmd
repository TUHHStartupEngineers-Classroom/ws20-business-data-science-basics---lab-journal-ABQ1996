---
title: "Journal (reproducible report)"
author: "Lionel Will"
date: "2020-11-23"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
    message: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```


# Section 1: Intro to the tidyverse

Last compiled: `r Sys.Date()`

Our Task was to analyse the bike sales data. After previously analyzing the data by the bike-type and manufacturing-year we now want to take a look at the number of sales sorted by their state in which they were sold in. Afterwards we want to take a look how these numbers changed over the last years.

Let's first take a look at the number of sales per state if you add up all the prevoius years together:

```{r}
# 1.0 Load libraries ----
  library(tidyverse)
  library(lubridate)
  library(writexl)
  #Read Excel Files
  library(readxl)

# 2.0 Importing Files ----
  bikes <- read_xlsx(path = "~/Documents/GitHub/ws20-business-data-science-basics---lab-journal-ABQ1996/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
  bike_shops <- read_xlsx(path = "~/Documents/GitHub/ws20-business-data-science-basics---lab-journal-ABQ1996/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
  bike_orders <- read_xlsx(path = "~/Documents/GitHub/ws20-business-data-science-basics---lab-journal-ABQ1996/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")

# 3.0 Examining Data ----
  glimpse(bike_orders)

# 4.0 Joining Data ----
  bike_orderlines_joined <- bike_orders %>% left_join(bikes, by = c("product.id"="bike.id")) %>% left_join(bike_shops, by = c("customer.id" = "bikeshop.id"))
  bike_orderlines_joined %>% glimpse()
  #glimpse(bike_orderlines_joined)
  
# 5.0 Wrangling Data ----
  bike_orderlines_joined %>% 
  select(category) %>%
  filter(str_detect(category, "^Mountain")) %>% 
  unique()

# 6.0 Business Insights ----
  #Separate the joined category category into 3 separate columns
  bike_orderlines_joined <- separate(bike_orderlines_joined, col=category, into = c("cat.1","cat.2","cat.3"), sep = " - ")
  #Calculate the total price of the order
  bike_orderlines_joined <- mutate(bike_orderlines_joined, total.price = quantity*price)
  
  bike_orderlines_wrangled <- bike_orderlines_joined
  
  #8.1 Which state sells the most of the bikes?
  bike_orderlines_bystate <- bike_orderlines_wrangled

  #Separate the joined location category into 2 separate columns
  bike_orderlines_bystate <- separate(bike_orderlines_wrangled, col=location, into = c("state","city"), sep = ", ")

  # Manipulate the data
  sales_by_state <- bike_orderlines_bystate
  
  # Select columns
  sales_by_state %>% 
  select(state, city, quantity) %>% 
    
  # Grouping by year and summarizing sales
  group_by(state) %>% 
  summarize(numberofsales = sum(quantity)) %>%

  # Step 2 - Visualize
    
  # Setup canvas with the columns year (x-axis) and sales (y-axis)
  ggplot(aes(x = state, y = numberofsales)) +
    
  # Geometries
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = numberofsales)) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    
  # Formatting
  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. 
  # Again, we have to adjust it for euro values
  labs(
      title    = "Revenue by state",
      subtitle = "Upward Trend",
      x = "", # Override defaults for x and y
      y = "Revenue"
    )
```

Interesting! Let's see if Bremen is still the city with the most sales if we additionally sort them by their manufacturing-year:


```{r plot, fig.width= 10, fig.height=5}
 #8.2 Which state sells the most of the bikes in which year?
  sales_by_stateandyear <- bike_orderlines_bystate
  
  # Select columns
  sales_by_stateandyear %>% 
  select(state, city, quantity, order.date) %>% 
  mutate(year = year(order.date)) %>%
    
  # Group by and summarize year and main catgegory
  group_by(state,year) %>%
  summarise(numberofsales_2 = sum(quantity)) %>%
  ungroup() %>% 
    
  # Set up x, y, fill
  ggplot(aes(x = state, y = numberofsales_2, fill = year)) +
    
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
    
  # Facet
  facet_wrap(~ year) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
      title = "Revenue by year and main category",
      subtitle = "Each product category has an upward trend",
      fill = "Main category" # Changes the legend name
    )
 
```

# Section 2: Data Aquisition
Let's gather some data from the pokemon universe. Maybe they can help with the bike sales all over the country. Who doesn't want to catch a fast running pokemon if he or she sees it on the streets?
```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(httr)
library(glue)

Pokemon_API <- function(path) {
  url <- modify_url(url = "https://pokeapi.co", path = glue("/api/v2/pokemon-species/{path}/"))
  resp_raw <- GET(url)
  stop_for_status(resp_raw) # automatically throws an error if a request did not succeed
  resp_JSON <- fromJSON(rawToChar(resp_raw$content), flatten) 
}
Pokemon_species <- Pokemon_API("5")
Pokemon_species
```

Well that was some interesting insight into the pokemon universe. Now let's shift back to the bike sales in order to determine the bike categories of the competition:
```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(httr)
library(glue)

#8.2 Get the the model names, categories and prices of the bikes from rosebikes.de
# 8.2.1 COLLECT PRODUCT FAMILIES ----
  url_home          <- "https://www.rosebikes.de/fahrrÃ¤der" 

  # Read in the HTML for the entire webpage
  html_home         <- read_html(url_home) 

  # Web scrape the ids for the families
  bike_family_cop <- html_home %>% 
  
  # Get the nodes for the families ...
  html_nodes(css = ".catalog-navigation__link") %>%
  # ...and extract the information of the id attribute
  html_attr('title') %>% 
    
  # Remove the product families SALE and BIKE-FINDER 
  discard(.p = ~stringr::str_detect(.x,"Sale|Bike-Finder")) %>%
    
  # Convert vector to tibble effectivly giving each entry a number
  enframe(name = "position", value = "family_class") %>%
    
  # Add a hashtag so we can get nodes of the categories by id (#)
  mutate(
    family_id = str_glue("#{family_class}")
  )
    
#8.2.2 COLLECT PRODUCT CATEGORIES URL ----
  
  # Combine all Ids to one string so that we will get all nodes at once
  # (seperated by the OR operator ",")
  #family_id_css <- bike_family_cop %>%
  #  pull(family_id) %>%
  #  stringr::str_c(collapse = ", ")
  
  # Extract the urls from the href attribute
  bike_category_cop <- html_home %>%
    
  # Select nodes by the ids
  html_nodes(css = ".catalog-navigation__link") %>%
  html_attr('href') %>%
  
  # Remove the product families SALE and BIKE-FINDER 
  discard(.p = ~stringr::str_detect(.x,"sale|zoovu")) %>%  
    
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
    
  # Add the domain, because we will get only the subdirectories
  mutate(
    url = glue("https://www.rosebikes.de{subdirectory}")
  ) %>%
    
  # Some categories are listed multiple times.
  # We only need unique values
  distinct(url)
  
#8.2.3 GET THE MODEL NAMES FROM THE URLs ----
    #First Category
    # select first bike category url
    bike_model_url1 <- bike_category_cop$url[1]
  
    # Get the URLs for the bikes of the first category
    html_bike_model_cop1  <- read_html(bike_model_url1) %>% 
  
    # Select nodes by the ids
    html_nodes(css = ".catalog-category-bikes__title") %>%
    html_text('catalog-category-bikes__title-text') %>%  
    
    # Convert vector to tibble
    enframe(name = "position", value = "name")
    
    #2nd Category
    # select first bike category url
    bike_model_url2 <- bike_category_cop$url[2]
    
    # Get the URLs for the bikes of the first category
    html_bike_model_cop2  <- read_html(bike_model_url2) %>% 
      
    # Select nodes by the ids
    html_nodes(css = ".catalog-category-bikes__title") %>%
    html_text('catalog-category-bikes__title-text') %>%  
      
    # Convert vector to tibble
    enframe(name = "position", value = "name")    
  
#8.2.4 GET THE MODEL PRICES FROM THE URLs ----
  #First Category
  # select first bike category url
  bike_model_url1 <- bike_category_cop$url[1]
  
  # Get the URLs for the bikes of the first category
  html_bike_model_price_cop1  <- read_html(bike_model_url1) %>% 
      
  # Select nodes by the ids
  html_nodes(css = ".catalog-category-bikes__price") %>%
  html_text('catalog-category-bikes__price-title') %>%  
    
  # Remove the query parameters of the URL (everything after the '?')
  str_remove(pattern = "ab ") %>%
  str_remove(pattern = "oder ab.*") %>%
    
  # Convert vector to tibble
  enframe(name = "position", value = "name")
    
  #2nd Category
  # select first bike category url
  bike_model_url2 <- bike_category_cop$url[2]
    
  # Get the URLs for the bikes of the first category
  html_bike_model_price_cop2  <- read_html(bike_model_url2) %>% 
      
  # Select nodes by the ids
  html_nodes(css = ".catalog-category-bikes__price") %>%
  html_text('catalog-category-bikes__price-title') %>%  
  
  # Remove the query parameters of the URL (everything after the '?')
  str_remove(pattern = "ab ") %>%
  str_remove(pattern = "oder ab.*") %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "name")    
  
  #Join the vectors together
  bike_cat1_cop <- left_join(html_bike_model_cop1,html_bike_model_price_cop1, by = c("position"="position"))
  bike_cat2_cop <- left_join(html_bike_model_cop2,html_bike_model_price_cop2, by = c("position"="position"))
  
  
  bike_category_cop
  bike_cat1_cop
  bike_cat2_cop
```
Well that's sad. There seems to be too much competition on the market in order to correlate the number of pokemons with an increase in bike sales.


# Section 3: Data Wrangling
Let's use the data from the USPTO website to find out a little bit about patents in the US. 

First let's take a look at the TOP 10 US companies in regards to the number of patents:
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)

#10.1. First Part: Which US company has the most patents? List the 10 US companies with the most granted patents ----

col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
  patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
  patent_p_assignee <- patent_p_assignee[-c(1), ]

combined_patent_assignee_data <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))

combined_patent_assignee_data_DT <- as.data.table(combined_patent_assignee_data) %>% 
    filter((type == 2)) %>% 
    group_by(organization) %>% 
    summarise(total = n()) %>% 
    ungroup() %>% 
    arrange(desc(total))
head(combined_patent_assignee_data_DT,10)
```
Now let's take a look at the TOP 10 US companies in 2019 in regards to their number of filed patents:
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
#10.2 What US company had the most patents granted in 2019? List the top 10 companies of 2019 ----

col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
patent_p_assignee <- patent_p_assignee[-c(1), ]

col_types_patent <- list(
  patent_id = col_character(),
  pat_type = col_character(),
  pat_number = col_character(),
  pat_country = col_character(),
  pat_date = col_date("%Y-%m-%d")
)

patent_list <- vroom(
  file       = "~/Dokumente (Offline)/patent.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_patent),
  col_types  = col_types_patent,
  na         = c("", "NA", "NULL")
  ) %>% 
  select(patent_id, pat_date)
patent_list <- patent_list[-c(1), ]

combined_patent_data_prev <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))
combined_patent_data <- left_join(combined_patent_data_prev,patent_list, by = c("patent_id"="patent_id"))

combined_patent_data_DT <- as.data.table(combined_patent_data) %>% 
  mutate(pat_date = year(pat_date)) %>% 
  filter((type == 2)) %>% 
  filter((pat_date == 2019)) %>% 
  group_by(organization) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
head(combined_patent_data_DT,10)
```
Lastly let's take a look at their USPTO tech main classes. What are the TOP 5 tech main classes of the TOP 10 companies filing patents in the US worldwide. 
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)


col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
patent_p_assignee <- patent_p_assignee[-c(1), ]

col_types_uspc <- list(
  nn3 = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character()
)

patent_uspc <- vroom(
  file       = "~/Dokumente (Offline)/uspc.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_uspc),
  col_types  = col_types_uspc,
  na         = c("", "NA", "NULL")
) %>% 
  select(patent_id,mainclass_id)
patent_uspc <- patent_uspc[-c(1), ]

combined_uspc_data_prev <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))
combined_uspc_data <- left_join(combined_uspc_data_prev,patent_uspc, by = c("patent_id"="patent_id"))


combined_uspc_data_companies_DT <- as.data.table(combined_uspc_data) %>% 
  group_by(organization) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
combined_uspc_data_companies_DT <- combined_uspc_data_companies_DT[-c(2), ]
combined_uspc_data_companies_DT <- head(combined_uspc_data_companies_DT,10)
combined_uspc_data_companies_DT

combined_uspc_patentdata_DT <- as.data.table(combined_uspc_data) %>% 
  group_by(organization) %>% 
  select(organization,mainclass_id) %>% 
  filter((organization == combined_uspc_data_companies_DT[1,1]|organization == combined_uspc_data_companies_DT[2,1]|organization == combined_uspc_data_companies_DT[3,1]|organization == combined_uspc_data_companies_DT[4,1]|organization == combined_uspc_data_companies_DT[5,1]|organization == combined_uspc_data_companies_DT[6,1]|organization == combined_uspc_data_companies_DT[7,1]|organization == combined_uspc_data_companies_DT[8,1]|organization == combined_uspc_data_companies_DT[9,1]|organization == combined_uspc_data_companies_DT[10,1])) %>% 
  group_by(mainclass_id) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
combined_uspc_patentdata_DT <- combined_uspc_patentdata_DT[-c(1), ]
head(combined_uspc_patentdata_DT,5)
```

# Section 4: Data Visualization
As the last part, let's take a look at our more recent global situation. How did the COVID-19 case number evolve throughout the year 2020?
```{r}
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
library(scales)
library(ggrepel)
library(maps)

# 1.1 COVID data preparation ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")


covid_data_tbl_DT <- as.data.table(covid_data_tbl) %>% 
  filter(countriesAndTerritories == "Germany"|countriesAndTerritories == "France"|countriesAndTerritories == "Spain"|countriesAndTerritories == "United_Kingdom"|countriesAndTerritories == "United_States_of_America") %>%  
  select("dateRep", "countriesAndTerritories", cases) %>%
  mutate(dateRep=dmy(dateRep)) %>%
  arrange(dateRep, desc()) %>% 
  group_by(countriesAndTerritories) %>% 
    mutate(cum_cases = cumsum(cases)) %>% 
  ungroup()

Cases_France <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "France") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Spain <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "Spain") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Germany <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "Germany") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_UK <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "United_Kingdom") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Europe <- left_join(Cases_France, Cases_Spain, by = c("dateRep" = "dateRep"))
Cases_Europe <- left_join(Cases_Europe, Cases_Germany, by = c("dateRep" = "dateRep"))
Cases_Europe <- left_join(Cases_Europe, Cases_UK, by = c("dateRep" = "dateRep")) %>% 
                    add_column("Europe",.after = 1) %>%
                    rename("France"  = "cum_cases.x",
                        "Spain" = "cum_cases.y",
                        "Germany" = "cum_cases.x.x" ,
                        "UK" = "cum_cases.y.y",
                          "countriesAndTerritories" = 2) %>% 
                    mutate(cum_cases = France+Spain+Germany+UK) %>% 
                    select(dateRep,countriesAndTerritories,cum_cases) %>% 
                    as.data.table()
covid_data_tbl_DT <- covid_data_tbl_DT%>% 
  select(dateRep,countriesAndTerritories, cum_cases)
  
covid_data_tbl_DT <- rbind(covid_data_tbl_DT,Cases_Europe) %>% 
  arrange(dateRep, desc())

#1.2 COVID data plot
covid_data_tbl_DT %>%
# Canvas
ggplot(aes(x = dateRep,y = cum_cases, color = countriesAndTerritories))+
# Geometries 
geom_line(size = 0.5, linetype = 1, aes(color=countriesAndTerritories)) + 
expand_limits(y = 0) +

labs(
  title = "COVID-19 confirmed cases worldwide",
  subtitle = "As of 31/10/2020. Europe only consists of countries shown with the execption of the USA.",
  x = "Year 2020",
  y = "Cumulatitve Cases"
)+   

theme_light()+
theme(legend.position  = "bottom", 
      legend.direction = "vertical") +
guides(col = guide_legend(ncol = 3,
       title = "Continent / Country",
       title.position = "left"
     )
  )+
scale_color_manual(values=c('grey50','red','blue','darkgreen','brown','orange'))+
scale_x_date(labels = date_format(("%b")),breaks = breaks_width("1 month")) +
scale_y_continuous(labels = scales::dollar_format(scale = 1/1000000, 
                                                    prefix = "",
                                                    suffix = " M")) +

geom_label_repel(aes(
  label=ifelse((dateRep == "2020-11-26" & countriesAndTerritories == "United_States_of_America") , as.character(cum_cases),'')), 
  nudge_x = -10,
  label.padding = unit(0.55, "lines"), # Rectangle size around label
  label.size = 0.35,
  point.padding = unit(0.55,"lines"),
  segment.color = 'orange',
  color = "white",
  fill="orange"
)+

geom_label_repel(aes(
  label=ifelse((dateRep == "2020-11-25" & countriesAndTerritories == "Europe") , as.character(cum_cases),'')), 
  nudge_x = -20,
  label.padding = unit(0.55, "lines"), # Rectangle size around label
  label.size = 0.35,
  point.padding = unit(0.55,"lines"),
  segment.color = 'grey50',
  color = "white",
  fill="grey50"
)
```

And which countries have the most death cases due to the corona virus in regards to their population?
```{r}
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
library(scales)
library(ggrepel)
library(maps)


worldmap_raw <- map_data("world")

# 1.1 COVID data preparation ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

covid_data_tbl_DT2 <- covid_data_tbl %>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
  ))

covid_data_tbl_DT2 <- covid_data_tbl_DT2 %>% 
  select(dateRep, deaths, popData2019, countriesAndTerritories) %>% 
  group_by(countriesAndTerritories) %>% 
  mutate(dateRep=dmy(dateRep)) %>%
  arrange(dateRep, desc()) %>% 
  mutate(cum_deaths = cumsum(deaths)) %>% 
  ungroup() %>% 
  filter(dateRep == "2020-11-25") %>%
  mutate(relative_deaths = (cum_deaths/popData2019)) %>% 
  select(countriesAndTerritories, relative_deaths)

worldmap <- worldmap_raw %>%
  select(long,lat,group,region)
worldmap_COVID <- left_join(worldmap, covid_data_tbl_DT2, by = c("region" = "countriesAndTerritories")) %>% 
mutate(relative_deaths_rounded = round(relative_deaths,4))

ggplot(worldmap_COVID, aes(map_id = region))+
  geom_map(aes(fill = relative_deaths_rounded), map = worldmap_COVID) +
  expand_limits(x = worldmap_COVID$long, y=worldmap_COVID$lat)+

  
labs(
  title = "Confirmed COVID-19 deaths relative to the size of the population",
  subtitle = "More than 1.2 Million confirmed COVID-19 deaths worldwide",
  x="",
  y=""
)+
  
theme_light()+
theme(legend.position  = "right", 
      legend.direction = "vertical",
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks = element_blank()) +
scale_fill_continuous(name = "Mortality Rate", type = "viridis")#, labels = scales::percent(scale = 1, decimal.mark = ".", suffix = " %"))
```


